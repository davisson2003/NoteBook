Dex2oat
Workflow:
* Dex2oat initialize.
* dex2oat.ParseArgs(argc, argv): 
  ** initialize locking, logging, gCommandLine.
  ** parse options from command.
  ** process options.
* dex2oat.OpenFile(), try to open the oat file for saving result.
* dex2oat.Setup()
* CompileImage() or CompileApp()
* dex2oat.ShutDown()


=================== DataStructure =============================================

-------------------- VerificationResults -------------------------------------
<src art/compiler/dex/verification_results.cc>


class VerificationResults {
  const CompilerOptions* const compiler_options_;
  typedef SafeMap<MethodReference, VerifiedMethod*, MethodReferenceComparator> VerifiedMethodMap;
  
  ReaderWriterMutex verified_methods_lock_;
  VerifiedMethodMap verified_methods_;

  ReaderWriterMutex  rejected_classes_lock_;
  std::set<ClassReference> rejected_classes_;
}
--------------------
Description:
The important DS in VerificationResults are :
* a Map between MethodReference and VerifiedMethod. holding the verified method.
* a set of ClassReference. holding the rejected classes.

===================== MethodReference / DexFileReference =============================
struct MethodReference {
  const DexFile* dex_file_;
  uint32_t dex_method_index;
}

struct DexFileReference {
  const DexFile* dex_file_;
  uint32_t index;
}

// A class is uniquely located by its DexFile and the class_defs_ table index in that dex file
typedef std::pair<const DexFile*, uint32_t> ClassReference;

-----------------
Desc:
MethodReference holds info where we can find the Method from dex file:
dexfile ptr and method index.


==================== VerifiedMethod =================================================
class VerifiedMethod {
  public:
    // typedefs
    typedef std::vector<uint32_t> SafeCastSet;
    // DevirtualizationMap, maps dex offset to MethodReference.
    typedef SafeMap<uint32_t, MethodReference> DevirtualizationMap;
    // DequickenMap, maps dex offset to DexFileReference (field/method idx). 
    typedef SafeMap<uint32_t, DexFileReference> DequickenMap; 

  private:
    std::vector<uint8_t> dex_gc_map_; // virtual register reference map.
    DervitualizationMap devirt_map_; 
    // Dequicken map is required for compiling quickened byte codes.
    // The quicken maps from dex PC to dex method/field index based on instruction.
    DequickenMap dequicken_map_;
    
    SafeCastSet safe_cast_set_;
    const uint32_t encountered_error_types_; 
    const bool has_runtime_throw_;

    // Copy of mapping generated by verifier of dex PCs of string init invocations
    // to the set of other registers that the receiver has been copied into. 
    const SafeMap<uint32_t, std::set<uint32_t>> string_init_pc_reg_map_;
}

-----------------
Desc:
VerifiedMethod holds all info about method after verification.
* dex_gc_map_:
    first 4 bytes:
    ** 1st byte : ref_bitmap_bytes (high 5 bits) : format (3bits)
    ** 2nd byte : ref_bitmap_bytes (low 8 bits)
    ** 3ed byte : number enteries (low 8 bits)
    ** 4th byte : number enteries (hign 8 bits)
    ** ((pc_bytes + ref_bitmap_bytes) * num_entries) 
    ** per entry: pc_bytes (1 or 2, determined by format) : bitmaps (1 for reference vr.) 

------------------
======================================================================================================
======================================================================================================
======================================================================================================

====================== QuickCompilerCallbacks =======================================================
<src art/compiler/dex/quick_compiler_callbacks.h>
<Derived from CompilerCallbacks>

class QuickCompilerCallbacks FINAL : public CompilerCallbacks {
 private:
   
   VerificationResults* const verification_results_;
   DexFileToMethodInlinerMap* const method_inliner_map_;
}

class CompilerCallbacks {
   enum class CallbackMode { 
      kCompileBootImage,
      kCompileApp
   }
 // Whether the compiler is creating a boot image.   
   const CallbackMode mode_;  
}

--------------------- DexFileToMethodInlinerMap -----------------------------------
<src art/compiler/dex/quick/dex_file_to_method_inliner_map.h>

/**
 * Map each DexFile to its DexFileMethodInliner.
 *
 * The method inliner is created and initialized the first time it's requested
 * for a particular DexFile.
 */
class DexFileToMethodInlinerMap {
 private:

    ReaderWriterMutex lock_;
    std::map<const DexFile*, DexFileMethodInliner*> inliners_
}

--------------------- DexFileMethodInliner ----------------------------------------
<src art/compiler/dex/quick/dex_file_method_inliner.h>

/**
 * Handles inlining of methods from a particular DexFile.
 *
 * Intrinsics are a special case of inline methods. The DexFile indices for
 * all the supported intrinsic methods are looked up once by the FindIntrinsics
 * function and cached by this class for quick lookup by the method index.
 *
 * TODO: Detect short methods (at least getters, setters and empty functions)
 * from the verifier and mark them for inlining. Inline these methods early
 * during compilation to allow further optimizations. Similarly, provide
 * additional information about intrinsics to the early phases of compilation.
 */
class DexFileMethodInliner {

    /**
     * To avoid multiple lookups of a class by its descriptor, we cache its
     * type index in the IndexCache. These are the indexes into the IndexCache
     * class_indexes array.
     */
    enum ClassCacheIndex : uint8_t {  // unit8_t to save space, make larger if needed
      kClassCacheFirst = 0,
      kClassCacheBoolean = kClassCacheFirst,
      kClassCacheByte,
      kClassCacheChar,
      kClassCacheShort,
      kClassCacheInt,
      kClassCacheLong,
      kClassCacheFloat,
      kClassCacheDouble,
      kClassCacheVoid,
      kClassCacheJavaLangByteArray,
      kClassCacheJavaLangCharArray,
      kClassCacheJavaLangIntArray,
      kClassCacheJavaLangObject,
      kClassCacheJavaLangRefReference,
      kClassCacheJavaLangString,
      kClassCacheJavaLangStringBuffer,
      kClassCacheJavaLangStringBuilder,
      kClassCacheJavaLangStringFactory,
      kClassCacheJavaLangDouble,
      kClassCacheJavaLangFloat,
      kClassCacheJavaLangInteger,
      kClassCacheJavaLangLong,
      kClassCacheJavaLangShort,
      kClassCacheJavaLangMath,
      kClassCacheJavaLangStrictMath,
      kClassCacheJavaLangThread,
      kClassCacheJavaNioCharsetCharset,
      kClassCacheLibcoreIoMemory,
      kClassCacheSunMiscUnsafe,
      kClassCacheJavaLangSystem,
      kClassCacheLast
    };

    /**
     * To avoid multiple lookups of a method name string, we cache its string
     * index in the IndexCache. These are the indexes into the IndexCache
     * name_indexes array.
     */
    enum NameCacheIndex : uint8_t {  // unit8_t to save space, make larger if needed
      kNameCacheFirst = 0,
      kNameCacheReverse =  kNameCacheFirst,
      kNameCacheReverseBytes,
      kNameCacheDoubleToRawLongBits,
      kNameCacheLongBitsToDouble,
      kNameCacheFloatToRawIntBits,
      kNameCacheIntBitsToFloat,
      kNameCacheAbs,
      kNameCacheMax,
      kNameCacheMin,
      kNameCacheSqrt,
      kNameCacheCeil,
      kNameCacheFloor,
      kNameCacheRint,
      kNameCacheRound,
      kNameCacheReferenceGetReferent,
      kNameCacheCharAt,
      kNameCacheCompareTo,
      kNameCacheEquals,
      kNameCacheGetCharsNoCheck,
      kNameCacheIsEmpty,
      kNameCacheIndexOf,
      kNameCacheLength,
      kNameCacheInit,
      kNameCacheNewStringFromBytes,
      kNameCacheNewStringFromChars,
      kNameCacheNewStringFromString,
      kNameCacheCurrentThread,
      kNameCachePeekByte,
      kNameCachePeekIntNative,
      kNameCachePeekLongNative,
      kNameCachePeekShortNative,
      kNameCachePokeByte,
      kNameCachePokeIntNative,
      kNameCachePokeLongNative,
      kNameCachePokeShortNative,
      kNameCacheCompareAndSwapInt,
      kNameCacheCompareAndSwapLong,
      kNameCacheCompareAndSwapObject,
      kNameCacheGetInt,
      kNameCacheGetIntVolatile,
      kNameCachePutInt,
      kNameCachePutIntVolatile,
      kNameCachePutOrderedInt,
      kNameCacheGetLong,
      kNameCacheGetLongVolatile,
      kNameCachePutLong,
      kNameCachePutLongVolatile,
      kNameCachePutOrderedLong,
      kNameCacheGetObject,
      kNameCacheGetObjectVolatile,
      kNameCachePutObject,
      kNameCachePutObjectVolatile,
      kNameCachePutOrderedObject,
      kNameCacheArrayCopy,
      kNameCacheNumberOfLeadingZeros,
      kNameCacheNumberOfTrailingZeros,
      kNameCacheRotateRight,
      kNameCacheRotateLeft,
      kNameCacheLast
    };

    /**
     * To avoid multiple lookups of a method signature, we cache its proto
     * index in the IndexCache. These are the indexes into the IndexCache
     * proto_indexes array.
     */
    enum ProtoCacheIndex : uint8_t {  // unit8_t to save space, make larger if needed
      kProtoCacheFirst = 0,
      kProtoCacheI_I = kProtoCacheFirst,
      kProtoCacheJ_J,
      kProtoCacheS_S,
      kProtoCacheD_D,
      kProtoCacheDD_D,
      kProtoCacheF_F,
      kProtoCacheFF_F,
      kProtoCacheD_J,
      kProtoCacheJ_D,
      kProtoCacheF_I,
      kProtoCacheI_F,
      kProtoCacheII_I,
      kProtoCacheI_C,
      kProtoCacheString_I,
      kProtoCache_Z,
      kProtoCache_I,
      kProtoCache_Object,
      kProtoCache_Thread,
      kProtoCacheJ_B,
      kProtoCacheJ_I,
      kProtoCacheJ_S,
      kProtoCacheJB_V,
      kProtoCacheJI_V,
      kProtoCacheJJ_J,
      kProtoCacheJJ_V,
      kProtoCacheJS_V,
      kProtoCacheObject_Z,
      kProtoCacheJI_J,
      kProtoCacheObjectJII_Z,
      kProtoCacheObjectJJJ_Z,
      kProtoCacheObjectJObjectObject_Z,
      kProtoCacheObjectJ_I,
      kProtoCacheObjectJI_V,
      kProtoCacheObjectJ_J,
      kProtoCacheObjectJJ_V,
      kProtoCacheObjectJ_Object,
      kProtoCacheObjectJObject_V,
      kProtoCacheCharArrayICharArrayII_V,
      kProtoCacheObjectIObjectII_V,
      kProtoCacheIICharArrayI_V,
      kProtoCacheByteArrayIII_String,
      kProtoCacheIICharArray_String,
      kProtoCacheString_String,
      kProtoCache_V,
      kProtoCacheByteArray_V,
      kProtoCacheByteArrayI_V,
      kProtoCacheByteArrayII_V,
      kProtoCacheByteArrayIII_V,
      kProtoCacheByteArrayIIString_V,
      kProtoCacheByteArrayString_V,
      kProtoCacheByteArrayIICharset_V,
      kProtoCacheByteArrayCharset_V,
      kProtoCacheCharArray_V,
      kProtoCacheCharArrayII_V,
      kProtoCacheIICharArray_V,
      kProtoCacheIntArrayII_V,
      kProtoCacheString_V,
      kProtoCacheStringBuffer_V,
      kProtoCacheStringBuilder_V,
      kProtoCacheLast
    };

  private:
    /**
     * The maximum number of method parameters we support in the ProtoDef.
     */
    static constexpr uint32_t kProtoMaxParams = 6;

    /**
     * The method signature (proto) definition using cached class indexes.
     * The return_type and params are used with the IndexCache to look up
     * appropriate class indexes to be passed to DexFile::FindProtoId().
     */
    struct ProtoDef {
      ClassCacheIndex return_type;
      uint8_t param_count;
      ClassCacheIndex params[kProtoMaxParams];
    };

    /**
     * The method definition using cached class, name and proto indexes.
     * The class index, method name index and proto index are used with
     * IndexCache to look up appropriate parameters for DexFile::FindMethodId().
     */
    struct MethodDef {
      ClassCacheIndex declaring_class;
      NameCacheIndex name;
      ProtoCacheIndex proto;
    };

    /**
     * The definition of an intrinsic function binds the method definition
     * to an Intrinsic.
     */
    struct IntrinsicDef {
      MethodDef method_def;
      InlineMethod intrinsic;
    };

    /**
     * Cache for class, method name and method signature indexes used during
     * intrinsic function lookup to avoid multiple lookups of the same items.
     *
     * Many classes have multiple intrinsics and/or they are used in multiple
     * method signatures and we want to avoid repeated lookups since they are
     * not exactly cheap. The method names and method signatures are sometimes
     * reused and therefore cached as well.
     */
    struct IndexCache {
      IndexCache();

      uint32_t class_indexes[kClassCacheLast - kClassCacheFirst];
      uint32_t name_indexes[kNameCacheLast - kNameCacheFirst];
      uint32_t proto_indexes[kProtoCacheLast - kProtoCacheFirst];
    };

    static const char* const kClassCacheNames[];
    static const char* const kNameCacheNames[];
    static const ProtoDef kProtoCacheDefs[];
    static const IntrinsicDef kIntrinsicMethods[];

    static const uint32_t kIndexNotFound = static_cast<uint32_t>(-1);
    static const uint32_t kIndexUnresolved = static_cast<uint32_t>(-2);

    ReaderWriterMutex lock_;
    /*
     * Maps method indexes (for the particular DexFile) to Intrinsic defintions.
     */
    SafeMap<uint32_t, InlineMethod> inline_methods_ GUARDED_BY(lock_);
    const DexFile* dex_file_;
};
---------------- InlineMethod ---------------------------------------------------
<src art/runtime/quick/inline_method_analyser.h>
struce InlineMethod {
  InlineMethodOpcode opcode;
  InlineMethodFlags flag;
  union {
    uint64_t data;
    InlineIGetIPutData ifield_data;
    InlineReturnArgData return_data;
  } d;
}
-------
Desc:
* InlineMethodOpcode contains intrinsics (like kIntrinsicIndexOf) and InlineOp (kInlineOpNop, ReturnArg, NonWideConst,IGet/IPut and StringInit)
* InlineMethodFlags : NoInlineMethodFlags, kInlineIntrinsic ,  kInlineSpecial 
* InlineIGetIPutData: 
    struct InlineIGetIPutData {
      // The op_variant below is DexMemAccessType but the runtime doesn't know that enumeration.
      uint16_t op_variant : 3;
      uint16_t method_is_static : 1;
      uint16_t object_arg : 4;
      uint16_t src_arg : 4;  // iput only
      uint16_t return_arg_plus1 : 4;  // iput only, method argument to return + 1, 0 = return void.
      uint16_t field_idx;
      uint32_t is_volatile : 1;
      uint32_t field_offset : 31;
    };
* InlineReturuArgData: 
    struct InlineReturnArgData {
      uint16_t arg;
      uint16_t is_wide : 1;
      uint16_t uint16_t is_object : 1;
      uint16_t reserved : 14;
      uint32_t reserved2;
    };
------------------------------------------------------------------------------------------
******************************************************************************************
******************************************************************************************
QuickCompilerCallbacks() Desc:
* it contains two components:
  ** verification_results_ contains:
    *** compiler_options_,
    *** VerifiedMethodMap verified_methods_; contains MethodReference(dexfile+index) and VerifiedMethods(meta info like gc map.) 
    *** rejected classes set.
  ** method_inliner_map, contains:
    *** inliners_ (map with dexfile and DexFileMethodInliner(contains dexfile and map<m_index, InlineMethod>))
    InlineMethod describes the method need to be inlined (opcode, flag, data).
    
===========================================================================================
===========================================================================================


===========================================================================================
=================== PrepareRuntimeOptions() ===============================================
<src dex2oat/dex2oat.cc>
* Runtime-option  --- value --- (dex2oat options)

* "-Xbootclasspath:" --- dex_filenames_ (--dex-file=)
* "-Xbootclasspath-locations:" --- dex_locations_ ("--dex-location=")
* "-Ximage:" --- boot_image_fileiname_ (--boot-image= or getenv(ANDROID_ROOT) + "/framework/boot.art")
* "compilercallbacks" --- callback
* "imageinstructionset" --- GetInstructionSetString(instruction_set)
* "-Xno-dex-file-fallback" --- nullptr (only if !BootImage())
* "-Xno-sig-chain" --- nullptr

==========================================================================================
=================== CreateRuntime(std::move(runtime_options)) ============================
<src dex2oat/dex2oat.cc>

1. Runtime::Create(std::move(runtime_options)) to create runtime with the option
   * create a new runtime instance: instance_ = new Runtime;
   * initialize the runtime:  instance_->Init();
     
     *** Memmap->Init();  mmaps_ = new MemMap if mmaps_ == nullptr,  dex2oat already init it. so nothing to do here.
     
     *** QuasiAtomic::Startup(); // just initalized the QuasiAtomic::gSwapMutexes vector, it has  kSwapMutexCount = 32 mutexs as elements.
         gSwapMutexes->push_back(new Mutex("QuasiAtomic stripe", kSwapMutexesLock))
     
     *** oat_file_manager_ = new OatFileManager; //create OatFileManager, it will set have_non_pic_oat_file_(false)
         **** OatFileManager is dealing all opeation of oatfile. the oatfile pointers returned from functions are always valid.
             ---------------- Class OatFileManager --------------------------------
             <src art/runtime/oat_file_manager.h >
             class OatFileManager {
               private: 
                  std::set<std::unique_ptr<const OatFile>> oat_files_ GUARDED_BY(Locks::oat_file_manager_lock_);
                  std::unordered_map<std::string, size_t> oat_file_count_ GUARDED_BY(Locks::oat_file_count_lock_);
                  bool have_non_pic_oat_file_;
             };
             ------------------- Desc ---------------------------------------------
             * oat_file_count_ is the pair of <oat_location, and reference count>, 
             RegisterOatFileLocation will inc the ref count, 
             UnRegisterOatFileLocation will dec the ref count. 
             when ref count dec to 0, will delete from the oat_file_count_ map.
             ======================================================================

     *** Monitor::Init(runtime_options.GetOrDefault(Opt::LockProfThreshold), runtime_options.GetOrDefault(Opt::HookIsSensitiveThread)); 
         Using the runtime options to init monitor.
         initialize the Monitor's  lock_profiling_threshold_ and  is_sensitive_thread_hook_ (all static)
         No monitor instance generated.

    *** Initialize parameters from runtime options: 
       boot_class_path_string_;  class_path_string_ ; properties_ <seems not used>;  compiler_callbacks_ <for dex2oat, quickCallback>;
       patchoat_executable_; must_relocate_; is_zygote; is_explicit_gc_disabled_; dex2oat_enabled_ <by default true>;  image_dex2oat_enabled_;
        vfprintf_ ; exit_ ;  abort_;  default_stack_size_;  stack_trace_file_; compiler_executable_; compiler_options_; image_compiler_options_;
        image_location_; max_spins_before_thin_lock_inflation_<50>;

    *** Monitor_list_ = new MonitorList;
       **** MonitorList 
           --------------------- Class MonitorList ---------------------
           <src art/runtime/monitor.h >
           class MonitorList {
             public: 
              typedef std::list<Monitor*, TrackingAllocator<Monitor*, kAllocatorTagMonitorList>> Monitors;
             private: 
              // During the sweeping we may free an object and on a seperate thread have an object created
              // using just freed memory. That object may then have it's lock word inflated and a monitor
              // created.If we allow new monitor registration during sweeping this monitor may be 
              // incorrectly freed as the object wasn't marked when sweep began.
               bool allow_new_monitors_ GUARDED_BY(monitor_list_lock_); 
               Mutex monitor_list_lock_ DEFAULT_MUTEX_ACQUIRED_AFTER;
               ConditionVariable monitor_add_condition_ GUARDED_BY(monitor_list_lock_);
               Monitors list_ GUARDED_BY(monitor_list_lock_);  
           }
          =================================================================

    *** monitor_pool_ = MonitorPool::Create();
       **** return new MonitorPool();
           --------------------- Class MonitorPool ---------------------
           <src art/runtime/monitor_pool.h >
           class MonitorPool {
             private: 
                // TODO: There are assumptions in the code that monitor addresses are 8B aligned (>>3).
                static constexpr size_t kMonitorAlignment = 8;
                // Size of a monitor, rounded up to a multiple of alignment.
                static constexpr size_t kAlignedMonitorSize = (sizeof(Monitor) + kMonitorAlignment - 1) &
                                                              -kMonitorAlignment;
                // As close to a page as we can get seems a good start.
                static constexpr size_t kChunkCapacity = kPageSize / kAlignedMonitorSize;
                // Chunk size that is referenced in the id. We can collapse this to the actually used storage
                // in a chunk, i.e., kChunkCapacity * kAlignedMonitorSize, but this will mean proper divisions.
                static constexpr size_t kChunkSize = kPageSize;
                // The number of initial chunks storable in monitor_chunks_. The number is large enough to make
                // resizing unlikely, but small enough to not waste too much memory.
                static constexpr size_t kInitialChunkStorage = 8U;
              
                // List of memory chunks. Each chunk is kChunkSize.
                Atomic<uintptr_t*> monitor_chunks_;
                // Number of chunks stored.
                size_t num_chunks_ GUARDED_BY(Locks::allocated_monitor_ids_lock_);
                // Number of chunks storable.
                size_t capacity_ GUARDED_BY(Locks::allocated_monitor_ids_lock_);
              
                // To avoid race issues when resizing, we keep all the previous arrays.
                std::vector<uintptr_t*> old_chunk_arrays_ GUARDED_BY(Locks::allocated_monitor_ids_lock_);
              
                typedef TrackingAllocator<uint8_t, kAllocatorTagMonitorPool> Allocator;
                Allocator allocator_;
              
                // Start of free list of monitors.
                // Note: these point to the right memory regions, but do *not* denote initialized objects.
                Monitor* first_free_ GUARDED_BY(Locks::allocated_monitor_ids_lock_);

           }
           =====================================================================================================

     *** thread_list = new ThreadList;
        **** ThreadList
        ----------------- Class ThreadList ------------------------------------------
        <src art/runtime/thread_list.h  >
        class ThreadList {
          
          std::bitset<kMaxThreadId> allocated_ids_ GUARDED_BY(Locks::allocated_thread_ids_lock_);

          // The actual list of all threads.
          std::list<Thread*> list_ GUARDED_BY(Locks::thread_list_lock_);

          // Ongoing suspend all requests, used to ensure threads added to list_ respect SuspendAll.
          int suspend_all_count_ GUARDED_BY(Locks::thread_suspend_count_lock_);
          int debug_suspend_all_count_ GUARDED_BY(Locks::thread_suspend_count_lock_);

          // Number of threads unregistering, ~ThreadList blocks until this hits 0.
          int unregistering_count_ GUARDED_BY(Locks::thread_list_lock_);

          // Thread suspend time histogram. Only modified when all the threads are suspended, so guarding
          // by mutator lock ensures no thread can read when another thread is modifying it.
          Histogram<uint64_t> suspend_all_historam_ GUARDED_BY(Locks::mutator_lock_);

          // Whether or not the current thread suspension is long.
          bool long_suspend_;
        }
        ======================================================================================================

     *** intern_table = new InternTable
        **** InternTable
        ------------------- Class InternTable--------------------------------------------------------------
        <src art/runtime/intern_table.h>
        /**
         * Used to intern strings.
         *
         * There are actually two tables: one that holds strong references to its strings, and one that
         * holds weak references. The former is used for string literals, for which there is an effective
         * reference from the constant pool. The latter is used for strings interned at runtime via
         * String.intern. Some code (XML parsers being a prime example) relies on being able to intern
         * arbitrarily many strings for the duration of a parse without permanently increasing the memory
         * footprint.
         */
        class InternTable {
       
           // Table which holds pre zygote and post zygote interned strings. There is one instance for
           // weak interns and strong interns.
           class Table {
             private:
               typedef HashSet<GcRoot<mirror::String>, GcRootEmptyFn, StringHashEquals, StringHashEquals,
                   TrackingAllocator<GcRoot<mirror::String>, kAllocatorTagInternTable>> UnorderedSet;
           
               // We call SwapPostZygoteWithPreZygote when we create the zygote to reduce private dirty pages
               // caused by modifying the zygote intern table hash table. The pre zygote table are the
               // interned strings which were interned before we created the zygote space. Post zygote is self
               // explanatory.
               UnorderedSet pre_zygote_table_;
               UnorderedSet post_zygote_table_;
          }; // Table
          
            bool image_added_to_intern_table_ GUARDED_BY(Locks::intern_table_lock_);
            bool log_new_roots_ GUARDED_BY(Locks::intern_table_lock_);
            ConditionVariable weak_intern_condition_ GUARDED_BY(Locks::intern_table_lock_);
            // Since this contains (strong) roots, they need a read barrier to
            // enable concurrent intern table (strong) root scan. Do not
            // directly access the strings in it. Use functions that contain
            // read barriers.
            Table strong_interns_ GUARDED_BY(Locks::intern_table_lock_);
            std::vector<GcRoot<mirror::String>> new_strong_intern_roots_
                GUARDED_BY(Locks::intern_table_lock_);
            // Since this contains (weak) roots, they need a read barrier. Do
            // not directly access the strings in it. Use functions that contain
            // read barriers.
            Table weak_interns_ GUARDED_BY(Locks::intern_table_lock_);
            // Weak root state, used for concurrent system weak processing and more.
            gc::WeakRootState weak_root_state_ GUARDED_BY(Locks::intern_table_lock_);

        }; // InternTable

   *** Init RuntimeOptions.
   
   *** Create Heap ---> new Heap()
   
   *** Configure JDWP options using ""-Xrunjdwp:_", "-agentlib:jdwp=_""
     **** Dbg::ConfigureJdwp(runtime_options.GetOrDefault(Opt::JdwpOptions));
     **** Init Dbg::  gJdwpOptions = jdwp_options; and gJdwpConfigured = true;

   *** set JIT options
   
   ***  lambda_box_table_ = MakeUnique<lambda::BoxTable>();
     **** // Allocate a global table of boxed lambda objects <-> closures. 

   *** Arena pool
     **** // Use MemMap arena pool for jit, malloc otherwise. 
     **** Malloc arenas are faster to allocate but can not be trim easily. 
     **** doesn't allocate any memory block.
         const bool use_malloc = IsAotCompiler();  
         arena_pool_.reset(new ArenaPool(use_malloc, false));
         if (IsAotCompiler() && Is64BitInstructionSet(kRuntimeISA)) { 
            low_4gb_arena_pool_.reset(new ArenaPool(false, true));
         } 
         linear_alloc_.reset(CreateLinearAlloc());  // new linera alloc based on the arena_pool or low4g_pool.

  *** BlockSignals() 
     **** blocks SIGPIPE, SIGQUIT, SIGUSR1

  *** InitPlatformSignalHandlers() 
     **** register signalhandler for different platform (android/linux)i

  *** fault handler for valgrind. see (https://android-review.googlesource.com/#/c/107232/)
     **** it will init FaultHandler, 
     **** and regist to faulthandler: <src runtime/arch/x86/fault_handler_x86.cc >
        ***** SuspensionHandler() if implicit_suspend_checks_ is true (false by default.)
             ** add to fault_handler's generated_code_hander
             ** Action() is for [generated code/other] check suspend by check thread's suspend_trigger.
        ***** StackOverflowHandler() if implicit_so_checks_ is true. (true by default for dex2oat, false for runtime.) 
             ** add to fault_handler's generated_code_hander
             ** Action() is for check stack over flow by "test eax, [esp+ -xxx]" where xxx is overflow area.
        ***** NullPointerHandler() if implicit_null_check is true; (true by default)
             ** add to fault_handler's generated_code_hander 
             ** Action() will set EIP to "uc->CTX_EIP = reinterpret_cast<uintptr_t>(EXT_SYM(art_quick_throw_null_pointer_exception));"
        *****  JavaStackTraceHandler () if kEnableJavaStackTraceHandler (fault by default)
             ** add to other_handler only
             ** the Action() if in generated code, will dump the java stack.

  ***  java_vm_ = new JavaVMExt(this, runtime_options);
     **** ---------------------- JavaVMExt ---------------------
          <src art/runtime/java_vm_ext.h>
          class JavaVMExt : public JavaVM {
            private: 

              Runtime* const runtime;
              
              // Used for testing. By default, we'll LOG(FATAL) the reason.
              void (*check_jni_abort_hook_)(void* data, const std::string& reason);
              void* check_jni_abort_hook_data_;

              // Extra Checking.
              bool check_jni;
              // for fast jni??
              bool force_copy;
              const bool tracing_enabled_;       

              // Extra diagnostics
               const std::string trace_; 

              // **** JNI Global References;
              ReaderWriterMutex globals_lock_ DEFAULT_MUTEX_ACQUIRED_AFTER; 

              // Not guarded by globals_lock since we sometimes use SynchronizedGet in Thread:::DecodeJObject.  
              IndirectReferenceTable globals_;

              // No lock annotation since UnloadNativeLibraries is called on libraries_ but locks the jni_libraries_lock_ internally.
              std::unique_ptr<Libraries> libraries_;

               // Used by -Xcheck:jni.                             
               const JNIInvokeInterface* const unchecked_functions_;

               // **** JNI Weak References;
               Mutex weak_globals_lock_ DEFAULT_MUTEX_ACQUIRED_AFTER; 

               // Since weak_globals_ contain weak roots, be careful not to 
               // directly access the object references in it. Use Get() with the 
               // Not guarded by weak_globals_lock since we may use SynchronizedGet in DecodeWeakGlobal.
                IndirectReferenceTable weak_globals_;  
                Atomic<bool> allow_accessing_weak_globals_; 

                ConditionVariable weak_globals_add_condition_ GUARDED_BY(weak_globals_lock_);
              };
          ---------------------- DESC:  ----------------------
          jvm at runtime side. contains runtime, global/weak global references.
          =====================================================

     **** ---------------------- JavaVM -----------------------
         <src libnativehelper/include/nativehelper/jni.h>
         class JavaVM {
            const struct JNIInvokeInterface* functions; 
         };
         ------------------------DESC:-------------------------
         It contains JNIInvokeInterface* for jni. represent the JVM to native.
         -----------------------------------------------------

     **** --------------------- JNIInvokeInterface ------------
        <src libnativehelper/include/nativehelper/jni.h >
         struct JNIInvokeInterface { 
            void*       reserved0; 
            void*       reserved1; 
            void*       reserved2;

           jint        (*DestroyJavaVM)(JavaVM*);
           jint        (*AttachCurrentThread)(JavaVM*, JNIEnv**, void*)
           jint        (*DetachCurrentThread)(JavaVM*);
           jint        (*GetEnv)(JavaVM*, void**, jint);
           jint        (*AttachCurrentThreadAsDaemon)(JavaVM*, JNIEnv**, void*);
         };
         ------------------------DESC:------------------------
         Interface to JVM from jni
         ==========================================================


      *** Thread::Startup()
        **** is_started_ = true;   
        **** new the resume_cond_ (with holding thread_suspend_count_lock_)
        **** call pthread_key_create, to create thread local data
            key: Thread::pthread_key_self_, 
            clean func: Thread::ThreadExitCallback()
            intialized value is nullptr.

      ***  Thread* self = Thread::Attach("main", false, nullptr, false); 
          **** if runtime is shutting down, return nullptr
          **** Runtime::Current()->StartThreadBirth(); //threads_being_born_++
          **** create a Thread instance: self = new Thread(as_daemon);
             ***** Internally it initialized: 
                ***** wait_mutex, wait_cond_,
                ***** tlsPtr.instrumentation_stack <queue of InstrumentationStackFrame>, which record this_object, method, return_pc, frame_id, interpret_entry_.
                ***** tlsPtr.name = kThreadNameDuringStartup "<native thread without managed peer>"
                ***** tlsPtr_.nested_signal_state =  static_cast<jmp_buf*>(malloc(sizeof(jmp_buf))); 
                ***** tls32_.state_and_flags.as_struct.flags = 0;
                ***** tls32_.state_and_flags.as_struct.state = kNative; // current state is Native.
                ***** init tlsPtr_.held_mutexesp[] to 0;  memset(&tlsPtr_.held_mutexes[0], 0, sizeof(tlsPtr_.held_mutexes)); 
                ***** fill tlsPtr_.rosalloc_runs( all KNumRosAllocThreadLocalSizeBrackets to dedicated_full_run_)
                ***** checkpoint func[] to nullptr:  tlsPtr_.checkpoint_functions[i] = nullptr; [totally 3]
                ***** suspend)barrier[] to nullptr:  tlsPtr_.active_suspend_barriers[i] = nullptr; [totally 3]
                ***** flip func to nullptr: tlsPtr_.flip_function = nullptr;
                ***** tlsPtr_.thread_local_mark_stack = nullptr;
                ***** tls32_.suspended_at_suspend_check = false;

         **** self->Init(runtime->GetThreadList(), runtime->GetJavaVM());
             ***** tlsPtr_.pthread_self = pthread_self() // get thread id.
             ***** SetUpAlternateSignalStack(); // nothing, bionic do it for us.
             ***** InitStackHwm(); // Setup the stack based on data from kernel
                ****** GetThreadStack(tlsPtr_.pthread_self, &read_stack_base, &read_stack_size, &read_guard_size)
                       * Invoke  CHECK_PTHREAD_CALL(pthread_attr_getstack, (&attributes, stack_base, stack_size), __FUNCTION__); to get
                       stack_base, stack_size.
                       * Invoke CHECK_PTHREAD_CALL(pthread_attr_getguardsize, (&attributes, guard_size), __FUNCTION__); to get guardsize.
                ****** tlsPtr.stack_begin = read_stack_base
                ****** tlsPtr.stack_size = read_stack_size
                ****** uint32_t min_stack = GetStackOverflowReservedBytes(kRuntimeISA) + kStackOverflowProtectedSize + 4*KB: 
                       * overflow reserved bytes (8K), protected region size (4K), another page(4k)
                ****** tlsPtr_.stack_end = tlsPtr_.stack_begin + GetStackOverflowReservedBytes(kRuntimeISA);
                ****** if (implicit_stack_check)   
                       *  tlsPtr_.stack_begin += read_guard_size + kStackOverflowProtectedSize; 
                       *  tlsPtr_.stack_end += read_guard_size + kStackOverflowProtectedSize; 
                       *  tlsPtr_.stack_size -= read_guard_size; 
                       *  InstallImplicitProtection(); // protect the stackoverflow region (kStackOverflowProtectedSize from begin,)
            ***** InitCpu()
                ****** for x86_32, creat LDT, and set selector to fs. set fs as thread*
                ****** for x86_64, set gs as thread*

            ***** InitTlsEntryPoints()
                ****** init all jni_entrypoints and quick_entrypoints to reinterpret_cast<uintptr_t>(UnimplementedEntryPoint); 
                ****** InitEntryPoints(&tlsPtr_.jni_entrypoints, &tlsPtr_.quick_entrypoints); 
                ****** init jpoints and qpoints: 
                    ***** JNI : art-jni_dlsym_lookup_stub: 
                    *****  SetQuickAllocEntryPoints_xxx():
                    ***** others for qpoints: 
                          // Cast
                          // DexCahe , Field, Array 
                          // JNI (qpoints->PJniMethodStart**)
                          // Locks, 
                          // Math
                          // Intrisics, (memcpy, string_compareto)
                          // Invocation,  qpoints->pQuickImtConflictTrampoline = art_quick_imt_conflict_trampoline; etc.
                          // Thread:  qpoints->pTestSuspend = art_quick_test_suspend;   
                          // Throws: qpoints->pDeliverException = art_quick_deliver_exception; (array_bounds, div_zero, no_such_method, null_ptr_exception, stack_overflow)
                          // Deoptimize:
                          // Read barrier:
               ****** RemoveSuspendTrigger():  tlsPtr_.suspend_trigger = reinterpret_cast<uintptr_t*>(&tlsPtr_.suspend_trigger); 
               ****** InitCardTable(): tlsPtr_.card_table = Runtime::Current()->GetHeap()->GetCardTable()->GetBiasedBegin();
               ****** InitTid() :  tls32_.tid = ::art::GetTid();
               ******  __get_tls()[TLS_SLOT_ART_THREAD_SELF] = this; 
               ****** tls32_.thin_lock_thread_id = thread_list->AllocThreadId(this); // set internal thead id. start from 0. also set allocated_ids_[i]=i, tid is i+1.

               ****** (if jni_env_ext <from parameter> != nullptr) tlsPtr_.jni_env = jni_env_ext; 
               ****** else tlsPtr_.jni_env = JNIEnvExt::Create(this, java_vm);
                      ----------------------------- JNIEnvExt ------------------------------------------------------------------
                      <src art/runtime/jni_env_ext.h >
                      class JNIEnvExt : public JNIEnv {
                         Thread* const self;
                         JavaVMExt* const vm;

                         // Cookie used when using the local indirect reference table.  
                         uint32_t local_ref_cookie;

                         // JNI local references. 
                         IndirectReferenceTable locals GUARDED_BY(Locks::mutator_lock_); 
                         
                         // Stack of cookies corresponding to PushLocalFrame/PopLocalFrame calls.
                         std::vector<uint32_t> stacked_local_ref_cookies;

                         // Frequently-accessed fields cached from JavaVM.
                         bool check_jni;

                         // How many nested "critical" JNI calls are we in?
                         int critical;

                         // Entered JNI monitors, for bulk exit on thread detach.
                         ReferenceTable monitors;

                         // Used by -Xcheck:jni. 
                         const JNINativeInterface* unchecked_functions;
                         
                        private:

                         // All locked objects, with the Java Caller stack frame that locked them. 
                         // Used in CheckJNI to ensure that only monitors locked in this native frame are 
                         // being unlocked, and that at the end all are unlocked.
                         std::vector<std::pair<uintptr_t, jobject>> locked_objects_;

                      }
                      ----------------------------- JNIEnv --------------------------------------------------------------------
                      <src libnativehelper/include/nativehelper/jni.h >
                      class _JNIEnv {
                        /* do not rename this; it does not seem to be entirely opaque */
                        const struct JNINativeInterface* functions;
                      }
                      ========================================================================================================

             ******  thread_list->Register(this);

         **** Runtime::Current()->EndThreadBirth();  threads_being_born_--; and  shutdown_cond_->Broadcast(Thread::Current()); when runtime shutting down and thread_being born_ = 0;

         **** self -> >InitStringEntryPoints();

         ****  self->SetState(kNative);    

         **** if(create_peer /*false by default*/) self->CreatePeer(thread_name, as_daemon, thread_group); 
         **** else SetThreadName:  self->tlsPtr_.name->assign(thread_name);  ::art::SetThreadName(thread_name); 


      ***  self->TransitionFromSuspendedToRunnable();  // Set us to runnable so tools using a runtime can allocate and GC by default
=======

2. runtime_->SetInstructionSet(instruction_set_) 
  * this set the instruction_set_ and 
  * initialize Runtime::callee_save_method_frame_infos_[i] to XXXCalleeSaveMethodFrameInfo(type), which actually create QuickMethodFrameInfo.
  Here i and type is the iteration of CalleeSaveType (kSaveAll, kRefsOnly, kRefsAndArgs)
  * init of XXXCalleeSaveMethodFrameInfo(type) inside create QuickMethodFrameInfo, and initialize the frame_size_in_bytes_, core_spill_mask_ and fp_spill_mask_ field due to the type parameter.
  ** Different ARCHs: (****** CALLING CONVENTION **********)
 
  ** ARM:
     1. Register Code: 
        R0 -- R15 ( from 0 to 15), TR = 9(thread register), FP = 11, IP = 12, SP = 13, LR = 14, PC=15
  
     2. CalleeSave Regs:
        kArmCalleeSaveAlwaysSpills: LR
        kArmCalleeSaveRefSpills: R5, R6, R7, R8, R10, R11.
        Args Regs: kArmCalleeSaveArgSpills: R1, R2, R3
        kArmCalleeSaveAllSpills: R4, R9
        
        kArmCalleeSaveFpArgSpills: S0--S15
        kArmCalleeSaveFpAllSpills: S16--S32
        others for FP are 0(means null)

     3. QuickMethodFrameInfo( frame_size_in_bytes, core_spills, fp_spills) 
        * frame_size_in_bytes is calculated by:
          (
           POPCOUNT(ArmCalleeSaveCoreSpills(type)) /* gprs */ +
           POPCOUNT(ArmCalleeSaveFpSpills(type)) /* fprs */ +
           1 /* Method* */
          ) * kArmPointerSize

        * CoreSpills: 
          kArmCalleeSaveAlwaysSpills | 
          kArmCalleeSaveRefSpills |
          (type == Runtime::kRefsAndArgs ? kArmCalleeSaveArgSpills : 0) |
          (type == Runtime::kSaveAll ? kArmCalleeSaveAllSpills : 0)
     
        * FpSpills:
          kArmCalleeSaveFpAlwaysSpills | 
          kArmCalleeSaveFpRefSpills | 
          (type == Runtime::kRefsAndArgs ? kArmCalleeSaveFpArgSpills: 0) | 
          (type == Runtime::kSaveAll ? kArmCalleeSaveFpAllSpills : 0)

  ** X86:
     1. Register Code: EAX, ECX, EDX, EBX, ESP, EBP, ESI, EDI (from 0 to 7)
        KNumberOfCpuRegisters = 8;
        kFirstByteUnsafeRegister = 4;
        kNoRegister = -1;

     2. CalleeSave Regs:
            kX86CalleeSaveRefSpills = EBP, ESI, EDI for "1" at bit mask.  
        Args Regs: 
            kX86CalleeSaveArgSpills = ECX, EDX, EBX for "1" at bit mask.
        XMM: 
            kX86CalleeSaveFpArgSpills = XMM0--XMM3

     3. QuickMethodFrameInfo( frame_size_in_bytes, core_spills, fp_spills) 
        * frame_size_in_bytes is calculated by:
           (
            POPCOUNT(X86CalleeSaveCoreSpills(type)) /* gprs */ + 
            2 * POPCOUNT(X86CalleeSaveFpSpills(type)) /* fprs */ + 
            1 /* Method* */
           ) * kX86PointerSize 

        * CoreSpills: 
        ** type == kSaveAll or kRefsOnly, it is kX86CalleeSaveRefSpills | 1 << art::x86::kNumberOfCpuRegisters) // for return address callee save.
        ** type == kRefsAndArgs, it is kX86CalleeSaveRefSpills | kX86CalleeSaveArgsSpills | 1 << kNumberOfCpuRegisters

        * FpSpills:
        kX86CalleeSaveFpArgsSpill for type == kRefsAndArgs, 0 otherwise.


  ** X86_64:
     1. Register Code: RAX, RCX, RDX, RBX, RSP, RBP, RSI, RDI, R8, R9, R10, R11, R12, R13, R14, R15 (from 0 to 15)
        kLastCpuRegister = 15,
        KNumberOfCpuRegisters = 16;
        kNoRegister = -1;

     2. CalleeSave Regs: 
            kX86_64CalleeSaveRefSpills = RBX, RBP, R12, R13, R14, R15 for "1" at bit mask.  
        Args Regs: 
            kX86_64CalleeSaveArgSpills = RSI, RDX, RCX, R8, R9 for "1" at bit mask.
        XMM: 
            kX86_64CalleeSaveFpSpills = XMM12 -- XMM15
            kX86_64CalleeSaveFpArgSpills = XMM0 -- XMM7

     3. QuickMethodFrameInfo( frame_size_in_bytes, core_spills, fp_spills) 
        * frame_size_in_bytes is calculated by:
           (
            POPCOUNT(X86_64CalleeSaveCoreSpills(type)) /* gprs */ + 
            POPCOUNT(X86_64CalleeSaveFpSpills(type)) /* fprs */ + 
            1 /* Method* */
           ) * kX86PointerSize 

        * CoreSpills: 
        ** type == kSaveAll or kRefsOnly, it is kX86_64CalleeSaveRefSpills | 1 << art::x86_64::kNumberOfCpuRegisters) // for return address callee save.
        ** type == kRefsAndArgs, it is kX86_64CalleeSaveRefSpills | kX86_64CalleeSaveArgsSpills | 1 << kNumberOfCpuRegisters

        * FpSpills:
        X86_64CalleeSaveFpSpills| X86_64CalleeSaveFpArgsSpill for type == kRefsAndArgs, 
        X86_64CalleeSaveFpSpills otherwise.


3. if calleeSaveMethod for calleeSaveType i not exist. create one:
   runtime_->SetCalleeSaveMethod(runtime_->CreateCalleeSaveMethod(), type);
   
   * Runtime::SetCalleeSaveMethod(Artmethod* method, CalleeSaveType type) just set the  callee_save_methods_[type] = method.
   * Runtime::CreateCalleeSaveMethod(): 
     ** auto* method = Runtime::Current()->GetClassLinker()->CreateRuntimeMethod();
       *** Allocate a 1 length MethodArray in LinearAlloc(),  ArtMethod* method = &method_array->At(0, method_size, method_alignment); 
       *** and method have no dexMethodIndex.  method->SetDexMethodIndex(DexFile::kDexNoIndex); 
       *** method content is not initialized, supposed to be all 0s.

     ** method->SetEntryPointFromQuickCompiledCodePtrSize(nullptr, pointer_size); 
       *** ((ArtMethod*)method)->ptr_sized_fields_-> entry_point_from_quick_compiled_code_ = nullptr
 -------------------------------------------------------------------------------------------------------------
 ----------- class ArtMethod --------------------------------------------------------------------------------
 
 class ArtMethod {

   protected: 

    // class we are part of.
    GcRoot<mirror::Class> declaring_class_;   
    
    // Access flags; low 16 bits are defined by spec.
    uint32_t access_flags_; 

    /* Dex file fields. The defining dex file is available via declaring_class_->dex_cache_ */

    // offset of the dex code item.
    uint32_t dex_code_item_offset;

    // index of the method_ids of the dex file for this method
    uint32_t dex_method_index;

     /* End of dex file fields. */   

    // Entry within a dispatch table for this method.
    // for static/direct methods the index is into the declaring_class_.directMethods.
    // for virtual methods the index is into the vtable.
    // for interface methods the index is into the iftable.
    uint16_t method_index;

    // The hotness we measure for the method. incremented by interpreter, Not atomic as it is ok to missing some inc.
    uint16_t hotness.
    
    // Fake padding fields get inserted here.

    // Must be the last fields in the method.
    // PACKED(4) is necessory for correctness of
    // RoundUp(OFFSETOF_MEMBER(ArtMethod, ptr_sized_fields_), pointer_size). 
    struct PACKED(4) PtrSizedFields {
     
      // Short cuts to declaring_class_->dex_cache_ member for fast compiled code access
      ArtMethod** dex_cache_resolved_methods_;

      // Short cuts to declaring_class_->dex_cache_ member for fast compiled code access
      GcRoot(mirror::Class>* dex_cache_resolved_types_;

      // Ptr to JNI function registered to this method. or a function to resolve the JNI function.
      // or the profiling data for non-native methods.
      void* entry_point_from_jni_;

      // Method dispatch from quick compiled code invokes this ptr which may cause bridging into the interpreter.
      void* entry_point_from_compiled_code;

    } ptr_sized_fields_;
 };

------------- Desc: ------------------
ArtMethod contains info the runtime execution/ compiler/JIT compiler needs.
if method->dex_method_idx ==  DexFile::kDexNoIndex; means it is runtime method. not created from java.

--------------------------------------------------------------------------------------------------------------

4. FixupDexCaches with runtime_->GetResolutionMethod();
 
